{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = os.path.join(os.getcwd(), 'hf_cache')\n",
    "\n",
    "from transformers import XLNetConfig, XLNetModel, XLNetTokenizer, XLNetLMHeadModel \n",
    "from datasets import load_dataset\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers.data.data_collator import DataCollatorForPermutationLanguageModeling\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = XLNetTokenizer(vocab_file='models/smiles_sp.model',\n",
    "                           do_lower_case=False,\n",
    "                           keep_accents=True\n",
    "                           )\n",
    "tokenizer_lc = XLNetTokenizer(vocab_file='models/smiles_sp.model',\n",
    "                           do_lower_case=True,\n",
    "                           keep_accents=True\n",
    "                           )\n",
    "tokenizer_ka = XLNetTokenizer(vocab_file='models/smiles_sp.model',\n",
    "                           do_lower_case=False,\n",
    "                           keep_accents=False\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-0250d14726bc71f8\n",
      "Reusing dataset csv (e:\\molnlp\\mol-prop\\hf_cache\\datasets\\csv\\default-0250d14726bc71f8\\0.0.0\\bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n",
      "100%|██████████| 1/1 [00:00<00:00, 64.00it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('csv', data_files=['data/ogb_molhiv/train_hiv.csv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "molecule = dataset['train'][1]['smiles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "molecule\n",
    "molecule_backup = molecule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "molecule = molecule.lower()\n",
    "molecule = molecule_backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O=S(=O)(O)CCS(=O)(=O)O\n",
      "[27, 23, 28, 10, 5, 4, 11, 5, 4, 56, 10, 5, 4, 10, 5, 4, 5, 0, 0]\n",
      "[67, 74, 23, 88, 10, 74, 4, 11, 74, 4, 339, 10, 74, 4, 10, 74, 4, 74, 0, 0]\n",
      "[27, 23, 28, 10, 5, 4, 11, 5, 4, 56, 10, 5, 4, 10, 5, 4, 5, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(molecule)\n",
    "print(tokenizer(molecule)['input_ids'])\n",
    "print(tokenizer_lc(molecule)['input_ids'])\n",
    "print(tokenizer_ka(molecule)['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[67, 28, 160, 0, 28, 3, 0, 0]\n",
      "['▁', 'S', 'CCNS', '<unk>', 'S', 'C', '<unk>', '<unk>']\n"
     ]
    }
   ],
   "source": [
    "tk = tokenizer('SCCNS%^SC')['input_ids']\n",
    "print(tk)\n",
    "print(tokenizer.convert_ids_to_tokens(tk))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.unk_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8133\n"
     ]
    }
   ],
   "source": [
    "unk_tok = tokenizer.unk_token_id\n",
    "count = 0\n",
    "for sample in dataset['train']:\n",
    "    enc = tokenizer(sample['smiles'])\n",
    "    found_unk = False\n",
    "    for tok in enc['input_ids']:\n",
    "        if tok == unk_tok:\n",
    "            if not found_unk:\n",
    "                found_unk = True\n",
    "        if found_unk and tok != unk_tok:\n",
    "            count += 1\n",
    "            break\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.mask_token_id"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "908b0ba9a178eea73af43abb00217a0aee98ad2ae4b0fcf81e35eb59597dfae8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('molnlp': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
