- Some commen options for NLP, such as capatilization, are very dangerous for this technique
- Some assumptions around NLP, like spaces have to be handled.
- Lots of errors with tokenizers.
  - XLNet seems to be completely broken.
  - Roberta/Bert Tokenizer doesn't work correctly when imported from tokenizers
  - Doesn't work with bla
- Bad memory usage on trainer/evaluator.
- Deberta does not work with fast tokenizer


Max bs1 layers Deberta: 24?